<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>King1Lfe</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-03-30T15:04:11.182Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jason</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/03/30/a/"/>
    <id>http://yoursite.com/2018/03/30/a/</id>
    <published>2018-03-30T06:40:48.758Z</published>
    <updated>2018-03-30T15:04:11.182Z</updated>
    
    <content type="html"><![CDATA[<p> Spark��Ⱥ��װ<br>1 ����spark��װ��<br>���ص�ַspark������<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a><br>����ʹ�� spark-2.0.2-bin-hadoop2.7�汾.</p><p>2 �滮��װĿ¼<br>/export/software<br>3 ��ѹ��װ��<br>tar -zxvf spark-2.0.2-bin-hadoop2.7.tgz<br>4 ������Ŀ¼<br>mv spark-2.0.2-bin-hadoop2.7 spark<br>5 �޸������ļ�<br>�����ļ�Ŀ¼�� /export/software/spark/conf<br>vi  spark-env.sh �޸��ļ�(�Ȱ�spark-env.sh.template������Ϊspark-env.sh)</p><p>#����java��������<br>export JAVA_HOME=/export/software/jdk1.7.0_67</p><p>#ָ��spark�ϴ�Master��IP<br>export SPARK_MASTER_HOST=node-1</p><p>#ָ��spark�ϴ�Master�Ķ˿�<br>export SPARK_MASTER_PORT=7077<br>vi  slaves �޸��ļ�(�Ȱ�slaves.template������Ϊslaves)<br>node-2<br>node-3<br>6 �������õ���������<br>ͨ��scp ���spark�İ�װĿ¼����������������<br>scp -r /export/software/spark node-2:/opt/bigdata<br>scp -r /export/software/spark node-3:/opt/bigdata</p><p>3.7 ����spark��������<br>��spark��ӵ���������,����������ݵ� /etc/profile<br>export SPARK_HOME=/opt/bigdata/spark<br>export PATH=$PATH:$SPARK_HOME/bin<br>ע����� source /etc/profile  ˢ������<br>3.7 ��spark</p><p>#�����ڵ�����spark<br>/opt/bigdata/spark/sbin/start-all.sh </p><p>3.8 ֹͣspark</p><p>#�����ڵ���ֹͣspark��Ⱥ<br>/opt/bigdata/spark/sbin/stop-all.sh </p><p>3.9 spark��web����<br>������spark��Ⱥ�󣬿���ͨ������ <a href="http://hdp-node-01:8080,�鿴spark��web���棬�鿴�����Ϣ��" target="_blank" rel="noopener">http://hdp-node-01:8080,�鿴spark��web���棬�鿴�����Ϣ��</a><br>�ġ� Spark HA�߿��ò���</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; Spark��Ⱥ��װ&lt;br&gt;1 ����spark��װ��&lt;br&gt;���ص�ַspark������&lt;a href=&quot;http://spark.apache.org/downloads.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hbase集群搭建</title>
    <link href="http://yoursite.com/2018/02/26/Hbase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/02/26/Hbase集群搭建/</id>
    <published>2018-02-26T05:59:28.000Z</published>
    <updated>2018-04-06T09:22:49.648Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>hbase是基于Google BigTable模型开发的，典型的key/value系统。是建立在hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写nosql的数据库系统。<br>它是Apache Hadoop生态系统中的重要一员，主要用于海量结构化和半结构化数据存储。</strong></p></blockquote><ul><li><p>1、登录hbase官网,选择Download, 下载Hbase,选择版本.自己用的是1.2.1</p></li><li><p>2、确保hadoop集群和zookeeper集群已经安装</p></li><li><p>3、上传hbase安装包</p></li><li><p>3、解压</p></li><li><p>4、配置hbase集群 :</p><blockquote><ul><li>拷贝hadoop的hdfs-site.xml和core-site.xml到hbase/conf下<br>cp /root/apps/hadoop-2.7.4/etc/hadoop/hdfs-site.xml $PWD<br>cp /root/apps/hadoop-2.7.4/etc/hadoop/core-site.xml $PWD  </li><li>修改 conf下的 hbase-env.sh<br>export JAVA_HOME=/export/servers/jdk<br>//使用外部的zk<br>export HBASE_MANAGES_ZK=false  </li><li>修改 hbase-site.xml<pre><code class="xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>         <span class="comment">&lt;!-- 指定hbase在HDFS上存储的路径 --&gt;</span>       <span class="tag">&lt;<span class="name">property</span>&gt;</span>                 <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>               <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node-1:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span>       <span class="tag">&lt;/<span class="name">property</span>&gt;</span>         <span class="comment">&lt;!-- 指定hbase是分布式的 --&gt;</span>       <span class="tag">&lt;<span class="name">property</span>&gt;</span>                 <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span>               <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>       <span class="tag">&lt;/<span class="name">property</span>&gt;</span>         <span class="comment">&lt;!-- 指定zk的地址，多个用“,”分割 --&gt;</span>       <span class="tag">&lt;<span class="name">property</span>&gt;</span>                 <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                 <span class="tag">&lt;<span class="name">value</span>&gt;</span>node-1:2181,node-2:2181,node-3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span>        <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></code></pre></li><li>修改 regionservers<pre><code>vi regionservers  node-2  node-3  </code></pre></li><li>修改 backup-masters来指定备用的主节点<br>vi backup-masters<br>node-2</li><li>配置环境变量:<br>vi /etc/profile<br>export HBASE_HOME=/root/apps/hbase<br>export PATH=$PATH:$HBASE_HOME/bin  </li><li>拷贝hbase到其他节点:<br>scp -r /root/apps/hbase  node-2:$PWD<br>scp -r /root/apps/hbase  ndoe-3:$PWD   </li><li>将配置好的HBase拷贝到每一个节点并同步时间:<br>ntpdate -u cn.pool.ntp.org    </li><li>启动所有的hbase进程:<br>首先启动zk集群:<br> zkServer.sh start<br>启动hdfs集群:<br> start-dfs.sh<br>启动hbase，在主节点node-1上运行：读取到配置文件了.所有的master都给启动,包括备份的master<br>start-hbase.sh </li></ul></blockquote></li><li><p>5、配置hbase集群:<br>通过浏览器访问hBase管理页面<br>  node-1:16010<br>  node-2:16010  </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;hbase是基于Google BigTable模型开发的，典型的key/value系统。是建立在hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写nosql的数据库系统。&lt;br&gt;它是Apache Hadoop生态系统中的重
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>scala+actor并发编程实现单机版本的WordCount(简易原理)</title>
    <link href="http://yoursite.com/2018/02/24/scala+actor%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0wordCount%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0%E5%99%A8/"/>
    <id>http://yoursite.com/2018/02/24/scala+actor并发编程实现wordCount单词计数器/</id>
    <published>2018-02-24T05:59:28.000Z</published>
    <updated>2018-03-29T06:31:31.248Z</updated>
    
    <content type="html"><![CDATA[<p>需求:<br>scala + actor并发编程 ,写一个单机版的WordCount，将多个文件作为输入，计算完成后将多个任务汇总，得到最终的结果。</p><p>思路:</p><blockquote><ul><li>通过loop +react 方式去不断的接受消息</li><li>利用case class样例类去匹配对应的操作</li><li>其中scala中提供了文件读取的接口Source,通过调用其fromFile方法去获取文件内容</li><li>将每个文件的单词数量进行局部汇总，存放在一个ListBuffer中</li><li>最后将ListBuffer中的结果进行全局汇总。</li></ul></blockquote><p>代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">import scala.actors.&#123;Actor, Future&#125;</span><br><span class="line">import scala.collection.mutable</span><br><span class="line">import scala.collection.mutable.ListBuffer</span><br><span class="line">import scala.io.Source</span><br><span class="line"></span><br><span class="line">//Actor 负责 解析每一个文件,把文件的结果返回</span><br><span class="line"></span><br><span class="line">// 定义存储数据的样例类</span><br><span class="line">//一个作为消息,说明提交文件的路径</span><br><span class="line">case class SumitTask(file:String);</span><br><span class="line">//一作保存每一个文件的单词数量</span><br><span class="line">case class ResultTask(result:Map[String,Int]);</span><br><span class="line"></span><br><span class="line">class WordCountTask extends Actor&#123;</span><br><span class="line">  override def act(): Unit = &#123;</span><br><span class="line">    loop &#123;</span><br><span class="line">      react&#123;</span><br><span class="line">      case SumitTask(file)</span><br><span class="line">      =&gt;</span><br><span class="line">      &#123;</span><br><span class="line">        //1,读取数据文件</span><br><span class="line">        val data: String = Source.fromFile(file).mkString;</span><br><span class="line">        //2,将文件数据进行按行切分,一行.在window下是 \r\n,在 linux下是 \n 在mac下是\r</span><br><span class="line">        val LineArray: Array[String] = data.split(&quot;\r\n&quot;);</span><br><span class="line">        //3,将每一行按空格切分(数据暂先按照空格来分),得到所欲单词</span><br><span class="line">        val words: Array[String] = LineArray.flatMap(_.split(&quot; &quot;));</span><br><span class="line">        //4,给每个单词统计标记为1个,单词 和 标记 1 组成元组</span><br><span class="line">        val wordMark: Array[(String, Int)] = words.map((_, 1));</span><br><span class="line">        //5,对相同的单词进行分组</span><br><span class="line">        val wordSameGrouop: Map[String, Array[(String, Int)]] = wordMark.groupBy(_._1);</span><br><span class="line">        //6,统计文件中单词出现的次数</span><br><span class="line">        val result: Map[String, Int] = wordSameGrouop.mapValues(_.length);</span><br><span class="line">        //7,结果,存储到样例类中,并返回</span><br><span class="line">        sender ! ResultTask(result);</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//WordCount对象负责把文件传给actor ,然后接受接受每个文件的单词统计,做最后汇总</span><br><span class="line">object WordCount&#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    var fileArr=Array(&quot;D:/aa.txt&quot;,&quot;D:/bb.txt&quot;,&quot;D:/cc.txt&quot;);</span><br><span class="line">    var FutureSet = new mutable.HashSet[Future[Any]]();</span><br><span class="line">    var resultList = new ListBuffer[ResultTask];</span><br><span class="line"></span><br><span class="line">    //循环遍历文件,发送消息,接受数据</span><br><span class="line">    for(file&lt;-fileArr)&#123;</span><br><span class="line">      val task = new WordCountTask;</span><br><span class="line">      task.start();</span><br><span class="line">      val reply:Future[Any] = task !! SumitTask(file);</span><br><span class="line">      //将每次发送的文件返回的数据保存到可变的HashSet中</span><br><span class="line">      FutureSet += reply;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //搜集整合所有文件返回的数据</span><br><span class="line">    while(FutureSet.size &gt; 0)&#123;</span><br><span class="line">      val completedFuture : mutable.HashSet[Future[Any]] = FutureSet.filter(_.isSet);</span><br><span class="line">      for( oneCompletedFuture &lt;- completedFuture)&#123;</span><br><span class="line">        val apply: Any = oneCompletedFuture.apply();</span><br><span class="line">        resultList+= apply.asInstanceOf[ResultTask];</span><br><span class="line">        FutureSet-=oneCompletedFuture;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //整合所有数据</span><br><span class="line">    var resultLast = resultList.map(_.result).flatten.groupBy(_._1).mapValues(x=&gt;x.foldLeft(0)(_+_._2));</span><br><span class="line">    print(resultLast);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;需求:&lt;br&gt;scala + actor并发编程 ,写一个单机版的WordCount，将多个文件作为输入，计算完成后将多个任务汇总，得到最终的结果。&lt;/p&gt;
&lt;p&gt;思路:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;通过loop +react 方式去不断的接受消息&lt;
      
    
    </summary>
    
    
  </entry>
  
</feed>
